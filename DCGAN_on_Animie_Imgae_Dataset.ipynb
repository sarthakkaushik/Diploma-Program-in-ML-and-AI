{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN on Animie Imgae Dataset.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMDum4/PR1eqWwkbXP1irLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthakkaushik/Diploma-Program-in-ML-and-AI/blob/main/DCGAN_on_Animie_Imgae_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "06NSvPPDDlJy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# Helper libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to mount google drive in case you are loading the data from your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm7Ds1lAEij_",
        "outputId": "51a0c566-30e3-4d86-88ed-84d0b4c988cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing Working Directory\n",
        "import os\n",
        " \n",
        "# Function to Get the current\n",
        "# working directory\n",
        "def current_path():\n",
        "    print(\"Current working directory before\")\n",
        "    print(os.getcwd())\n",
        "    print()\n",
        " \n",
        " \n",
        "# Driver's code\n",
        "# Printing CWD before\n",
        "\n",
        "\n",
        "current_path()\n",
        " \n",
        "# Changing the CWD\n",
        "data_path = '/gdrive/MyDrive/Machine Learning Datasets/Animie Dataset'\n",
        "os.chdir(data_path)\n",
        " \n",
        "# Printing CWD after\n",
        "current_path()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mqTu1avJBtB",
        "outputId": "0761a353-671e-41a0-f1d9-fdbd61164a69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory before\n",
            "/gdrive\n",
            "\n",
            "Current working directory before\n",
            "/gdrive/MyDrive/Machine Learning Datasets/Animie Dataset\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUqayyG-KBys",
        "outputId": "d6b635b9-aafb-410f-8f94-f15f7cea2fdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/Machine Learning Datasets/Animie Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fxUWuOTNkqb",
        "outputId": "3698baad-a631-4b08-c66b-1a83139fe7bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  9 23:37:16 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # importing required modules\n",
        "# from zipfile import ZipFile\n",
        "  \n",
        "# # specifying the zip file name\n",
        "# file_name = \"ZipAnimie.zip\"\n",
        "  \n",
        "# # opening the zip file in READ mode\n",
        "# with ZipFile(file_name, 'r') as zip:\n",
        "#     # printing all the contents of the zip file\n",
        "#     # zip.printdir()\n",
        "  \n",
        "#     # extracting all the files\n",
        "#     print('Extracting all the files now...')\n",
        "#     zip.extractall()\n",
        "#     print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GruFmskHXGt",
        "outputId": "ad56f880-e5b5-4d91-e0da-35292b78115f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9tn0BwKDLDdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading and PreParing the data"
      ],
      "metadata": {
        "id": "ZwSnSGmDLD2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path=glob(\"/gdrive/MyDrive/Machine Learning Datasets/Animie Dataset/data/*\")\n",
        "\n",
        "#Defining Global Variable \n",
        "IMG_H = 64\n",
        "IMG_W = 64\n",
        "IMG_C = 3  ## Change this to 1 for grayscale."
      ],
      "metadata": {
        "id": "by82mrQzOGoo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img)\n",
        "    img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = (img - 127.5) / 127.5\n",
        "    return img\n",
        "\n",
        "def tf_dataset(images_path, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(images_path)\n",
        "    dataset = dataset.shuffle(buffer_size=10240)\n",
        "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "QlRdfN1PLBhX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUFFER_SIZE = 60000\n",
        "buffer_size=10240\n",
        "EPOCHS = 10\n",
        "noise_dim = 128\n",
        "num_examples_to_generate = 16\n",
        "batch_size = 128\n",
        "images_dataset = tf_dataset(image_path, batch_size)\n",
        "image_path2='gdrive/MyDrive/Machine Learning Datasets/Animie Dataset/data/'"
      ],
      "metadata": {
        "id": "1vR2sCcTLUOh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_path2='gdrive/MyDrive/Machine Learning Datasets/Animie Dataset/data/'"
      ],
      "metadata": {
        "id": "LxDfVhCnZPZa"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset= images_dataset"
      ],
      "metadata": {
        "id": "khe_7g2YQzZl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd9TiOffQYNt",
        "outputId": "7f799cec-87b5-4b35-d8a0-5b6c1ae8b1d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: (None, 64, 64, None), types: tf.float32>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#our obj is here is that Genrator take bunch noise of 1x128 dim and outputs 28x28x1 image from that noise\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(128,))) #7*7*256 this has been taken from tesnflow doc after they have hypertuned the parameter\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU()) # to avoid dead neouron \n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    \n",
        "    # assert is used to debug\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    \n",
        "    # Refer: https://medium.com/@vaibhavshukla182/why-do-we-need-conv2d-transpose-2534cd2a4d98\n",
        "    # Conv2DTranspose <=> DeConvolution\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 3)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9cbHeN6xPGyP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "generator.summary()\n",
        "# keras.utils.plot_model(generator, 'generator.png', show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOKot_ueNmYI",
        "outputId": "0fbb2771-ffff-4477-a106-9a517582160e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 12544)             1605632   \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 12544)            50176     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_49 (LeakyReLU)  (None, 12544)             0         \n",
            "                                                                 \n",
            " reshape_13 (Reshape)        (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_31 (Conv2D  (None, 7, 7, 128)        819200    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_50 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_32 (Conv2D  (None, 14, 14, 64)       204800    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 14, 14, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_51 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_33 (Conv2D  (None, 28, 28, 3)        4800      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,685,376\n",
            "Trainable params: 2,659,904\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal([1, 128]) # creating 1 randome sample of 100 dimension\n",
        "\n",
        "# forward-pass through the generator \n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "p38iP2neRLHx",
        "outputId": "77e51e30-1644-4297-d2ad-dd98e51180ea"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7dd03d9c10>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 3]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ax4MwRoBRzen"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator_model()\n",
        "discriminator.summary()\n",
        "# keras.utils.plot_model(discriminator, 'discriminator.png', show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n6FMhwXR2El",
        "outputId": "013f0a54-992a-4767-a9ba-2bc40019d1d8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 14, 14, 64)        4864      \n",
            "                                                                 \n",
            " leaky_re_lu_54 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 7, 7, 128)         204928    \n",
            "                                                                 \n",
            " leaky_re_lu_55 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 6273      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 216,065\n",
            "Trainable params: 216,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decision = discriminator(generated_image) # forward-pass. model not yet trained\n",
        "print (decision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPQjH0cGR6S3",
        "outputId": "2af644b5-4308-4bd4-e4e1-bd2ff0659666"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-0.00173885]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss: Binary CrossEntropy <=> log-loss\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "XGq6mi57R9gB"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the discrimanator,  0 => fake and 1 => real image\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output) # tf.ones creates a tensor of ones\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) #tf.zeros creates a tensor of zeros\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "ce_SkZB5SADL"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "oyWq-f3ISDcx"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "df7Cv9F7SF5P"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator) # what objects to store in a ckpt"
      ],
      "metadata": {
        "id": "TgcT35oySIjL"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# We will reuse these test_random_vectors overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "test_random_vectors = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "print(test_random_vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGRApIkOSMFn",
        "outputId": "7b277f18-12d7-4455-c226-bf60ccd2960a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\" into a TF Graph\n",
        "@tf.function \n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "metadata": {
        "id": "t8jj-ubcSp-0"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input): #model is nothing but a genrator model\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5) # genrator image will be between [-1,1], hence here we are denormalizing the data\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LOsymFOlSt_E"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             test_random_vectors)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           test_random_vectors)"
      ],
      "metadata": {
        "id": "U5kRE7tYSyMj"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "tUn2sq6tS0ZR",
        "outputId": "9b2674be-e4e0-4cac-f3af-35387006c125"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-d152560ca122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-9c26dc5eb2be>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Produce images for the GIF as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-82-dbfe4d30056e>\", line 10, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_22\" is incompatible with the layer: expected shape=(None, 28, 28, 3), found shape=(128, 64, 64, 3)\n"
          ]
        }
      ]
    }
  ]
}