{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarthak-LSH from Scratch Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CID_27ISaRpJ"
      },
      "source": [
        "# Implement LSH from scratch\n",
        "\n",
        "In this assignment, you will implement LSH from scratch and predict the labels of the test data. You will then verify the correctness of the your implementation using a \"grader\" function/cell (provided by us) which will match your implmentation.\n",
        "\n",
        "The grader fucntion would help you validate the correctness of your code. \n",
        "\n",
        "Please submit the final Colab notebook in the classroom ONLY after you have verified your code using the grader function/cell.\n",
        "\n",
        "\n",
        "**NOTE: DO NOT change the \"grader\" functions or code snippets written by us.Please add your code in the suggested locations.**\n",
        "\n",
        "Ethics Code:\n",
        "1. You are welcome to read up online resources to implement the code. \n",
        "2. You can also discuss with your classmates on the implmentation over Slack.\n",
        "3. But, the code you wirte and submit should be yours ONLY. Your code will be compared against other stduents' code and online code snippets to check for plagiarism. If your code is found to be plagiarised, you will be awarded zero-marks for all assignments, which have a 10% wieghtage in the final marks for this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR49rnr6ibOX"
      },
      "source": [
        "## Reading the data from csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXNLRg93cPJN",
        "outputId": "891984bd-fad9-4ad4-aeff-0854090a9d06"
      },
      "source": [
        "# Code to mount google drive in case you are loading the data from your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "oA1hSk2odHUy",
        "outputId": "db7a2dc6-632a-44e8-fa11-bb9e9c9fb5f2"
      },
      "source": [
        "# Loading data from csv file\n",
        "import pandas as pd\n",
        "data_path = '/gdrive/MyDrive/PGD Assignment Notebooks/lsh_assignment_data.csv'\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>NaN</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>NaN</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>NaN</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>NaN</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>NaN</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "0              tech  tv future in the hands of viewers with home th...\n",
              "1          business  worldcom boss  left books alone  former worldc...\n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3             sport  yeading face newcastle in fa cup premiership s...\n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "...             ...                                                ...\n",
              "2220            NaN  cars pull down us retail figures us retail sal...\n",
              "2221            NaN  kilroy unveils immigration policy ex-chatshow ...\n",
              "2222            NaN  rem announce new glasgow concert us band rem h...\n",
              "2223            NaN  how political squabbles snowball it s become c...\n",
              "2224            NaN  souness delight at euro progress boss graeme s...\n",
              "\n",
              "[2225 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju7KEiUTbzPV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKHb7v5edUiU",
        "outputId": "ba308590-282a-4aae-a8cf-2d3c2026f7cc"
      },
      "source": [
        "# Data Overiview\n",
        "df['category'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sport            509\n",
              "business         508\n",
              "politics         415\n",
              "tech             399\n",
              "entertainment    384\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcpy_Nrnig9V"
      },
      "source": [
        "### Creating Train and Test Datasets\n",
        "\n",
        "Note that the labels for test data will not be present in the dataset and hence they are mentioned as NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncAK-oHFeKbS"
      },
      "source": [
        "# The last 10 rows in the csv file are query points, so loading them into test data.\n",
        "# And loading the reamining points to train_data for which labels are given.\n",
        "\n",
        "train_data = df.iloc[:-10,:]\n",
        "test_data = df.iloc[-10:,:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "bs7uYx-1fh66",
        "outputId": "77aa06cc-e915-4b7e-f7ab-89ce30ff07d1"
      },
      "source": [
        "# For train_data here the labels are in the column named \"category\".\n",
        "train_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2210</th>\n",
              "      <td>politics</td>\n",
              "      <td>teens  know little  of politics teenagers ques...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2211</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>lopez misses uk charity premiere jennifer lope...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2212</th>\n",
              "      <td>business</td>\n",
              "      <td>christmas shoppers flock to tills shops all ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>tech</td>\n",
              "      <td>progress on new internet domains by early 2005...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2214</th>\n",
              "      <td>business</td>\n",
              "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2215 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "0              tech  tv future in the hands of viewers with home th...\n",
              "1          business  worldcom boss  left books alone  former worldc...\n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3             sport  yeading face newcastle in fa cup premiership s...\n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "...             ...                                                ...\n",
              "2210       politics  teens  know little  of politics teenagers ques...\n",
              "2211  entertainment  lopez misses uk charity premiere jennifer lope...\n",
              "2212       business  christmas shoppers flock to tills shops all ov...\n",
              "2213           tech  progress on new internet domains by early 2005...\n",
              "2214       business  bush budget seeks deep cutbacks president bush...\n",
              "\n",
              "[2215 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "fc-SORtAfgqI",
        "outputId": "bd60c1c1-7684-4025-ff89-b2dd35b10f09"
      },
      "source": [
        "test_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2215</th>\n",
              "      <td>NaN</td>\n",
              "      <td>junk e-mails on relentless rise spam traffic i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2216</th>\n",
              "      <td>NaN</td>\n",
              "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2217</th>\n",
              "      <td>NaN</td>\n",
              "      <td>rings of steel combat net attacks gambling is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2218</th>\n",
              "      <td>NaN</td>\n",
              "      <td>davies favours gloucester future wales hooker ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2219</th>\n",
              "      <td>NaN</td>\n",
              "      <td>beijingers fume over parking fees choking traf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>NaN</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>NaN</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>NaN</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>NaN</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>NaN</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category                                               text\n",
              "2215      NaN  junk e-mails on relentless rise spam traffic i...\n",
              "2216      NaN  top stars join us tsunami tv show brad pitt  r...\n",
              "2217      NaN  rings of steel combat net attacks gambling is ...\n",
              "2218      NaN  davies favours gloucester future wales hooker ...\n",
              "2219      NaN  beijingers fume over parking fees choking traf...\n",
              "2220      NaN  cars pull down us retail figures us retail sal...\n",
              "2221      NaN  kilroy unveils immigration policy ex-chatshow ...\n",
              "2222      NaN  rem announce new glasgow concert us band rem h...\n",
              "2223      NaN  how political squabbles snowball it s become c...\n",
              "2224      NaN  souness delight at euro progress boss graeme s..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1jeyM0emKOw"
      },
      "source": [
        "## Custom Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU6mt5wq3Oyg"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "  1. Read in the train_data.\n",
        "  2. Vectorize train_data using sklearns built in tfidf vectorizer.\n",
        "  3. Ignore unigrams and make use of both **bigrams & trigrams** and also limit the **max features** to **4000** and **minimum document frequency** to **10**.\n",
        "  4. After the tfidf vectors are generated as mentioned above, next task is to generate random hyperplanes.\n",
        "  5. Generate **5 random hyperplanes**. And generate the hyperplanes using a random normal distribution with **mean zero and variance 1**. \n",
        "  6. We have set the **numpy random seed to zero**, please do not change it. And then you can make use of **np.random.normal** to generate the vectors for hyperplanes.\n",
        "  7. As mentioned in the course videos, compute the hash function and also the corresponding hash table for it.\n",
        "  8. Once the hash table is generated now take in each of the query points from the test data.\n",
        "  9. Vectorize those query points using the same tfidf vectorizer as mentioned above.\n",
        "  10. Now use the hash function on this query point and fetch all the similar data points from the hashtable.\n",
        "  11. Use cosine similarity to compute **11-Nearest Neighbours** from the list of data points obtained in the above step.\n",
        "  12. Take a majority vote among the 11-Nearest Neighbours and predict the class label for the query point in the test data.\n",
        "  13. **In case of a tie** in the obtained labels from nearest neighbours, you can pick a label after sorting all the labels **alphabetically**(A-Z), i.e. for example labels starting with A would get more preference than labels starting with Z.\n",
        "  14. Repeat steps 9 to 13 for all the points in the test data and then finally return a list with all the predicted labels.\n",
        "  15. Note that there are a total of 10 data points in the test data so the final list you return should be of length 10.\n",
        "  16. Also note that the cosine similarity function should be written from scratch, you should not directly make use of existing libraries.\n",
        "  17. Please use the formula of cosine similarity as explained in the course videos, you can make use of numpy or scipy to calculate dot or norm or transpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YECivOCWfvGn"
      },
      "source": [
        "# Please implement this fucntion and write your code wherever asked. Do NOT change the code snippets provided by us.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def predictLabels (test_data):\n",
        "  \"\"\"\n",
        "  Given the test_data, return the labels for all the rows in the test data.\n",
        "  Follow the step by step instructions mentioned above.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  np.random.seed(0)\n",
        "\n",
        "  ##############################################################\n",
        "  ####  Write YOUR CODE BELOW  as per the above instructions ###\n",
        "  ##############################################################\n",
        "  #Creating Tain Test data of small size\n",
        "  train_data = df.iloc[:-10,:]\n",
        "  test_data = df.iloc[-10:,:]\n",
        "  # train_data = df.iloc[:10,:]\n",
        "  # test_data = df.iloc[-10:,:]\n",
        "\n",
        "  #Impoerting Library\n",
        "  import pandas as pd\n",
        "  \n",
        "  import nltk\n",
        "  import string\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  from sklearn.feature_extraction.text import TfidfTransformer\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  ######################################################################################\n",
        "  #Training the TFIDF vector\n",
        "  tfidf_vect=TfidfVectorizer(ngram_range=(2,3),max_features=4000,min_df=10)\n",
        "  train_tfidf=tfidf_vect.fit_transform(train_data['text'].values)\n",
        "  # print(train_tfidf.toarray().shape)\n",
        "  # print(train_tfidf.toarray())\n",
        "  Train_processed=train_tfidf.toarray()\n",
        "  # pd.DataFrame(train_tfidf.toarray(),columns = tfidf_vect.get_feature_names())\n",
        "  # print(Train_processed)\n",
        "\n",
        "  ######################################################################################\n",
        "  #Generate 5 random hyperplanes\n",
        "  Hyperplanes=[]\n",
        "  np.random.seed(0)\n",
        "  for i in range(5):\n",
        "    Hyperplanes.append(np.random.normal(0,1,4000))\n",
        "\n",
        "  # print(\"Length of Hyper plane =\", len(Hyperplanes))\n",
        "  # print(\"Shape of fist hyper plane=\", Hyperplanes[0])\n",
        "\n",
        "  ######################################################################################\n",
        "  #Creating HashValue Function to create Hash function\n",
        "  def hashvalue(vect):\n",
        "    hasvalues=[]\n",
        "    for i in range(len(Hyperplanes)):\n",
        "      hasvalues.append((np.dot(vect,Hyperplanes[i]) > 0).astype('int'))\n",
        "\n",
        "    return tuple(hasvalues)\n",
        "\n",
        "  ########################################################################################\n",
        "  #Creating Hash Table Function\n",
        "\n",
        "  hash_dict={}\n",
        "  idx=0 # for storing the index position instead of storing tfidf of a point which will be a sparse amtrix\n",
        "  for text in Train_processed:\n",
        "    hash_key=hashvalue(text)\n",
        "    #Check if the key is present in the dict.\n",
        "    if hash_key not in hash_dict.keys():  \n",
        "      hash_dict[hash_key]=list()\n",
        "    hash_dict[hash_key].append(idx)\n",
        "    idx+=1\n",
        "    \n",
        "  #Print the key value pair\n",
        "  # for pairs in hash_dict.items():\n",
        "  #   print(pairs)\n",
        "  ########################################################################################\n",
        "  # Vectorazing my Test data\n",
        "  Test_tfidf=tfidf_vect.transform(test_data['text'].values)\n",
        "  Test_processed=Test_tfidf.toarray()\n",
        "  print(Test_processed.shape)\n",
        "  # print(Test_processed[0])\n",
        "\n",
        "  ########################################################################################\n",
        "\n",
        "  #Now use the hash function on this query point and fetch all the similar data points from the hashtable.\n",
        "#Use cosine similarity to compute 11-Nearest Neighbours from the list of data points obtained in the above step\n",
        "#Defining Cosin Similarity Function\n",
        "  def CosinSim(Query_pt,neigh_vect):  \n",
        "    cosin_sim=np.dot(Query_pt,neigh_vect)/(np.linalg.norm(Query_pt)*np.linalg.norm(neigh_vect))\n",
        "    return cosin_sim\n",
        "  KNN_11=[]\n",
        "  Final_ans=[]\n",
        "  for i in range(Test_processed.shape[0]):\n",
        "    Query_pt=Test_processed[i] #Fetching the query point\n",
        "    #looping through all query point in Test data\n",
        "    test_HV=hashvalue(Test_processed[i]) #Creating Hashvalue of eacch query point\n",
        "    # print(test_HV)\n",
        "    # print(hash_dict[test_HV_1])\n",
        "    All_CosSim=[]\n",
        "    neigh_pt=hash_dict[test_HV] #fetching all the neighbouring point for the query point hash value\n",
        "    for indx,pt in enumerate(neigh_pt):\n",
        "      # print(indx,pt)\n",
        "      neigh_vect=Train_processed[pt]# for each index value in neighbouring point we are finding correspondicg vector\n",
        "      \n",
        "      cosim= CosinSim(Query_pt,neigh_vect) #Finding the cosin similarity between the query point and each neighbouring point ( using loop indx)\n",
        "      All_CosSim.append((pt,cosim))\n",
        "    \n",
        "    # print(\"unsorted = \",All_CosSim)\n",
        "    Sorted_AllCosSim=(sorted(All_CosSim, key = lambda x: x[1],reverse=True))[:12] # KNN usin cosin sim for KNN=11\n",
        "    temp=[train_data['category'][x[0]] for x in Sorted_AllCosSim ] \n",
        "    Final_ans.append(max(sorted(temp),key=temp.count))\n",
        "    KNN_11.append(temp)\n",
        "\n",
        "\n",
        "  return Final_ans\n",
        "  \n",
        "\n",
        "# print(\"KNN_11-\",KNN_11)\n",
        "# print(\"Final ans-\",Final_ans)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5fWcpOS-aiI"
      },
      "source": [
        "#Creating Tain Test data of small size\n",
        "train_data = df.iloc[:-10,:]\n",
        "test_data = df.iloc[-10:,:]\n",
        "# train_data = df.iloc[:10,:]\n",
        "# test_data = df.iloc[-10:,:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeFxUQctEstB"
      },
      "source": [
        "#Create TFidf vector\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKwvhOQmFLgN",
        "outputId": "6488e1cd-94cb-42df-a24b-dd49f6f5369f"
      },
      "source": [
        "#Training the TFIDF vector\n",
        "tfidf_vect=TfidfVectorizer(ngram_range=(2,3),max_features=4000,min_df=10)\n",
        "train_tfidf=tfidf_vect.fit_transform(train_data['text'].values)\n",
        "print(train_tfidf.toarray().shape)\n",
        "# print(train_tfidf.toarray())\n",
        "Train_processed=train_tfidf.toarray()\n",
        "# pd.DataFrame(train_tfidf.toarray(),columns = tfidf_vect.get_feature_names())\n",
        "print(Train_processed)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2215, 4000)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBNrjhdMM-A2",
        "outputId": "f2c34f22-eefa-4513-8967-7fc60cef8cf2"
      },
      "source": [
        "#Generate 5 random hyperplanes\n",
        "Hyperplanes=[]\n",
        "np.random.seed(0)\n",
        "for i in range(5):\n",
        "  Hyperplanes.append(np.random.normal(0,1,4000))\n",
        "\n",
        "print(\"Length of Hyper plane =\", len(Hyperplanes))\n",
        "print(\"Shape of fist hyper plane=\", Hyperplanes[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Hyper plane = 5\n",
            "Shape of fist hyper plane= [ 1.76405235  0.40015721  0.97873798 ... -0.03057244  1.57708821\n",
            " -0.8128021 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-DFFiBQGimZ"
      },
      "source": [
        "#Creating HashValue Function to create Hash function\n",
        "def hashvalue(vect):\n",
        "  hasvalues=[]\n",
        "  for i in range(len(Hyperplanes)):\n",
        "    hasvalues.append((np.dot(vect,Hyperplanes[i]) > 0).astype('int'))\n",
        "\n",
        "  return tuple(hasvalues)\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deYFYc3bSTzr",
        "outputId": "f47d72f0-35ab-4ff1-d3d8-2f87de71cf39"
      },
      "source": [
        "#TESTINGGGGGGGGGGGGGGGGGGGGG\n",
        "x=Train_processed[0]\n",
        "x"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtfJ2wohIY_4"
      },
      "source": [
        "# #Creating Hash Table Function\n",
        "\n",
        "# #Creating Empty dictionary\n",
        "# hash_dict={}\n",
        "# for text in Train_processed:\n",
        "#   hash_key=hashvalue(text)\n",
        "#   #Check if the key is present in the dict.\n",
        "#   if hash_key not in hash_dict.keys():\n",
        "#     # hash_dict.update({hash_key,text})\n",
        "#     #Below is other way\n",
        "#     hash_dict[hash_key]=[]\n",
        "#   hash_dict[hash_key]=text\n",
        "\n",
        "# print(hash_dict)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmsjnwv0BI_0",
        "outputId": "ebb0618c-e424-4f2c-e28f-ce58ff84ce51"
      },
      "source": [
        "#Creating Hash Table Function\n",
        "\n",
        "#Creating Empty dictionary\n",
        "hash_dict={}\n",
        "idx=0 # for storing the index position instead of storing tfidf of a point which will be a sparse amtrix\n",
        "for text in Train_processed:\n",
        "  hash_key=hashvalue(text)\n",
        "  #Check if the key is present in the dict.\n",
        "  if hash_key not in hash_dict.keys():  \n",
        "    hash_dict[hash_key]=list()\n",
        "  hash_dict[hash_key].append(idx)\n",
        "  idx+=1\n",
        "  \n",
        "#Print the key value pair\n",
        "for pairs in hash_dict.items():\n",
        "  print(pairs)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((0, 1, 1, 0, 1), [0, 36, 110, 113, 139, 169, 198, 278, 295, 412, 512, 549, 565, 573, 574, 688, 696, 712, 839, 887, 977, 1021, 1071, 1185, 1227, 1243, 1266, 1273, 1381, 1383, 1433, 1463, 1495, 1500, 1503, 1526, 1550, 1584, 1814, 1849, 1864, 2032, 2040, 2051, 2131, 2137, 2151])\n",
            "((1, 1, 1, 1, 1), [1, 28, 54, 74, 92, 117, 118, 135, 146, 168, 172, 205, 242, 334, 388, 449, 521, 531, 532, 533, 540, 576, 615, 616, 655, 722, 781, 892, 935, 1087, 1091, 1140, 1210, 1220, 1274, 1335, 1343, 1358, 1379, 1430, 1564, 1569, 1727, 1785, 1811, 1829, 1840, 1898, 1938, 1949, 1994, 2030, 2083, 2140, 2208])\n",
            "((1, 0, 0, 0, 0), [2, 27, 32, 68, 73, 219, 230, 234, 244, 346, 399, 400, 415, 479, 496, 519, 558, 670, 687, 737, 758, 760, 773, 817, 880, 889, 922, 932, 952, 1049, 1095, 1126, 1156, 1169, 1201, 1365, 1396, 1446, 1513, 1557, 1588, 1624, 1653, 1668, 1703, 1728, 1729, 1787, 1792, 1796, 1870, 1879, 1893, 2069, 2074, 2135, 2176])\n",
            "((0, 1, 0, 0, 0), [3, 18, 185, 186, 195, 314, 326, 398, 424, 571, 604, 645, 672, 678, 695, 873, 921, 1136, 1180, 1183, 1255, 1283, 1354, 1363, 1373, 1426, 1488, 1531, 1762, 1764, 1769, 1808, 1824, 1859, 1919, 2013, 2015, 2018, 2058, 2103, 2143])\n",
            "((1, 0, 0, 1, 1), [4, 44, 56, 90, 133, 143, 173, 175, 183, 200, 212, 222, 240, 297, 299, 316, 358, 361, 372, 404, 413, 430, 463, 467, 494, 520, 555, 595, 632, 650, 664, 677, 769, 778, 801, 816, 818, 822, 858, 869, 872, 882, 895, 910, 965, 969, 976, 978, 980, 996, 1026, 1035, 1044, 1050, 1069, 1098, 1108, 1130, 1135, 1244, 1348, 1356, 1388, 1448, 1501, 1506, 1512, 1530, 1543, 1567, 1597, 1606, 1700, 1704, 1706, 1723, 1733, 1752, 1799, 1809, 1826, 1841, 1853, 1887, 1903, 1911, 1918, 1923, 1925, 1967, 1972, 1975, 1978, 1989, 2006, 2023, 2027, 2053, 2054, 2109, 2141, 2174, 2198, 2207])\n",
            "((0, 1, 0, 1, 0), [5, 9, 12, 55, 177, 182, 215, 264, 292, 328, 349, 441, 448, 453, 517, 554, 570, 598, 647, 660, 681, 786, 805, 821, 859, 860, 1063, 1151, 1206, 1413, 1611, 1647, 1685, 1698, 1738, 1784, 1803, 1830, 1839, 1901, 1905, 1950, 1957, 1979, 1999, 2028, 2122, 2148, 2164, 2211])\n",
            "((0, 1, 1, 0, 0), [6, 65, 97, 122, 145, 279, 282, 409, 446, 507, 524, 600, 621, 747, 857, 940, 959, 1028, 1054, 1102, 1106, 1257, 1366, 1400, 1440, 1541, 1559, 1690, 1722, 1734, 1737, 1802, 1812, 1866, 1929, 2008, 2157])\n",
            "((0, 0, 1, 1, 0), [7, 33, 61, 76, 89, 100, 111, 126, 178, 184, 191, 211, 237, 276, 290, 302, 305, 307, 340, 348, 461, 518, 581, 606, 620, 675, 684, 691, 693, 710, 829, 893, 913, 924, 928, 938, 946, 986, 1012, 1039, 1079, 1104, 1115, 1137, 1147, 1152, 1207, 1248, 1249, 1252, 1276, 1304, 1307, 1357, 1369, 1392, 1403, 1435, 1459, 1466, 1494, 1520, 1540, 1591, 1604, 1618, 1620, 1663, 1667, 1692, 1746, 1747, 1767, 1791, 1795, 1806, 1815, 1851, 1857, 1869, 1882, 1915, 1927, 1942, 1956, 1997, 2024, 2038, 2060, 2071, 2076, 2082, 2106, 2123, 2158, 2173, 2206, 2209, 2212])\n",
            "((1, 0, 1, 1, 0), [8, 13, 35, 42, 82, 85, 95, 108, 112, 115, 120, 132, 150, 157, 159, 166, 204, 217, 249, 308, 310, 311, 322, 338, 355, 362, 364, 411, 428, 445, 480, 489, 500, 503, 525, 551, 562, 626, 651, 682, 703, 705, 766, 810, 856, 881, 906, 916, 923, 937, 954, 955, 1025, 1042, 1059, 1125, 1141, 1142, 1170, 1191, 1195, 1202, 1230, 1242, 1256, 1289, 1322, 1340, 1374, 1387, 1393, 1418, 1454, 1527, 1562, 1577, 1607, 1619, 1636, 1669, 1672, 1705, 1720, 1744, 1759, 1771, 1788, 1856, 1863, 1894, 1906, 1907, 1940, 1947, 1953, 1955, 1965, 1970, 1995, 2009, 2036, 2066, 2086, 2092, 2104, 2144, 2156, 2187])\n",
            "((1, 1, 0, 0, 1), [10, 137, 283, 353, 384, 426, 563, 700, 753, 763, 782, 806, 813, 995, 997, 1038, 1161, 1197, 1217, 1219, 1233, 1262, 1360, 1376, 1436, 1462, 1493, 1507, 1580, 1601, 1621, 1659, 1696, 1753, 1755, 1837, 1845, 2007, 2025, 2077, 2079, 2119, 2127, 2191])\n",
            "((1, 1, 1, 1, 0), [11, 57, 210, 224, 289, 475, 544, 578, 591, 629, 729, 738, 754, 849, 852, 1008, 1188, 1281, 1294, 1301, 1311, 1330, 1476, 1489, 1496, 1519, 1538, 1590, 1613, 1629, 1648, 1765, 1786, 1789, 1910, 1962, 1988, 2063, 2070, 2096, 2192])\n",
            "((1, 1, 0, 1, 0), [14, 21, 24, 58, 121, 148, 154, 158, 165, 187, 238, 342, 432, 472, 501, 509, 515, 528, 535, 542, 543, 635, 683, 697, 726, 830, 991, 999, 1030, 1077, 1099, 1146, 1164, 1296, 1390, 1407, 1485, 1502, 1546, 1642, 1670, 1715, 1773, 1794, 1858, 1936, 1996, 2056, 2057, 2091, 2177])\n",
            "((0, 0, 0, 1, 0), [15, 37, 48, 49, 64, 71, 72, 80, 104, 106, 128, 138, 152, 161, 174, 188, 197, 268, 269, 286, 324, 333, 337, 347, 363, 371, 387, 390, 403, 416, 418, 431, 456, 460, 484, 492, 539, 575, 633, 709, 717, 744, 751, 756, 767, 770, 777, 785, 831, 862, 875, 879, 886, 897, 907, 947, 950, 951, 966, 974, 988, 998, 1002, 1003, 1007, 1033, 1034, 1055, 1057, 1070, 1083, 1119, 1124, 1162, 1165, 1171, 1172, 1174, 1198, 1218, 1235, 1271, 1280, 1284, 1288, 1332, 1367, 1372, 1404, 1422, 1423, 1431, 1455, 1460, 1497, 1498, 1537, 1542, 1592, 1646, 1655, 1664, 1683, 1701, 1743, 1760, 1775, 1776, 1807, 1810, 1813, 1827, 1834, 1847, 1890, 1920, 1931, 1937, 1948, 1954, 1992, 2003, 2011, 2031, 2047, 2094, 2147, 2149, 2199, 2204])\n",
            "((0, 1, 1, 1, 1), [16, 63, 116, 123, 149, 162, 164, 247, 252, 291, 293, 374, 455, 462, 495, 529, 564, 583, 597, 657, 666, 668, 692, 702, 720, 790, 794, 837, 864, 967, 1032, 1072, 1120, 1221, 1260, 1277, 1355, 1380, 1384, 1419, 1428, 1441, 1539, 1551, 1565, 1585, 1643, 1711, 1713, 1754, 1780, 1804, 1828, 1836, 1848, 1933, 1944, 1983, 2026, 2080, 2085, 2125, 2129, 2134, 2169])\n",
            "((0, 0, 0, 1, 1), [17, 19, 26, 29, 45, 47, 77, 86, 94, 114, 151, 180, 181, 190, 202, 208, 220, 241, 250, 261, 287, 318, 321, 336, 354, 377, 382, 392, 397, 439, 443, 452, 491, 516, 547, 548, 553, 580, 582, 608, 618, 623, 630, 639, 641, 654, 663, 669, 699, 701, 715, 736, 741, 759, 812, 845, 848, 896, 905, 909, 931, 941, 943, 964, 1037, 1045, 1048, 1061, 1062, 1066, 1073, 1074, 1085, 1088, 1111, 1155, 1178, 1216, 1229, 1254, 1272, 1282, 1291, 1292, 1295, 1299, 1329, 1336, 1338, 1385, 1401, 1424, 1429, 1444, 1453, 1457, 1471, 1482, 1487, 1490, 1521, 1522, 1528, 1533, 1536, 1545, 1568, 1570, 1589, 1593, 1598, 1630, 1718, 1751, 1781, 1793, 1805, 1818, 1820, 1855, 1871, 1892, 1904, 1913, 1922, 1926, 1973, 1974, 1981, 2021, 2034, 2065, 2078, 2088, 2090, 2112, 2118, 2120, 2160, 2161, 2163, 2201])\n",
            "((1, 0, 0, 1, 0), [20, 25, 41, 43, 87, 99, 102, 130, 136, 171, 216, 218, 262, 272, 304, 325, 330, 367, 380, 383, 396, 437, 438, 458, 497, 506, 522, 527, 546, 557, 577, 628, 640, 698, 708, 734, 742, 746, 749, 768, 775, 779, 798, 799, 811, 815, 819, 838, 840, 904, 972, 975, 981, 982, 984, 1046, 1081, 1112, 1133, 1144, 1167, 1184, 1203, 1239, 1287, 1314, 1321, 1352, 1386, 1391, 1412, 1420, 1432, 1467, 1469, 1470, 1511, 1517, 1529, 1561, 1574, 1605, 1628, 1631, 1678, 1679, 1736, 1739, 1801, 1817, 1822, 1924, 1928, 1939, 1946, 1982, 1987, 2004, 2017, 2050, 2064, 2139, 2150, 2168, 2172, 2175, 2180, 2181, 2190, 2200])\n",
            "((1, 0, 0, 0, 1), [22, 38, 39, 79, 119, 147, 179, 223, 231, 251, 263, 271, 313, 335, 341, 369, 376, 407, 419, 440, 560, 567, 572, 611, 667, 851, 884, 894, 903, 914, 962, 987, 1051, 1068, 1107, 1127, 1138, 1157, 1181, 1182, 1192, 1212, 1246, 1263, 1305, 1323, 1342, 1421, 1434, 1458, 1483, 1515, 1534, 1578, 1587, 1626, 1635, 1660, 1695, 1825, 1844, 1867, 1945, 2002, 2033, 2039, 2059, 2098, 2099, 2154, 2178, 2179, 2203, 2214])\n",
            "((0, 1, 1, 1, 0), [23, 96, 105, 153, 239, 243, 315, 401, 405, 468, 511, 587, 680, 694, 771, 780, 809, 847, 855, 876, 963, 973, 1029, 1113, 1175, 1223, 1224, 1378, 1408, 1416, 1461, 1481, 1576, 1665, 1691, 1899, 1930, 2022, 2037, 2171, 2213])\n",
            "((1, 0, 1, 1, 1), [30, 59, 103, 141, 199, 226, 232, 233, 235, 245, 246, 254, 259, 267, 296, 345, 385, 386, 421, 427, 442, 454, 470, 490, 534, 545, 673, 674, 706, 713, 723, 757, 796, 814, 844, 870, 883, 953, 960, 1000, 1005, 1013, 1036, 1056, 1100, 1109, 1128, 1163, 1194, 1261, 1290, 1300, 1312, 1326, 1353, 1368, 1397, 1445, 1475, 1478, 1544, 1579, 1608, 1634, 1637, 1640, 1654, 1673, 1686, 1725, 1749, 1761, 1816, 1861, 1876, 1897, 1900, 1912, 1977, 1984, 1985, 1986, 2081, 2084, 2089, 2105, 2116, 2126, 2153, 2186])\n",
            "((0, 0, 0, 0, 1), [31, 107, 125, 160, 203, 213, 228, 236, 255, 258, 300, 319, 389, 393, 394, 423, 429, 466, 469, 477, 498, 537, 559, 579, 584, 614, 617, 689, 724, 728, 732, 774, 793, 797, 832, 890, 911, 912, 917, 949, 958, 992, 994, 1011, 1052, 1084, 1094, 1110, 1131, 1176, 1193, 1196, 1204, 1214, 1225, 1231, 1234, 1238, 1247, 1275, 1333, 1339, 1347, 1371, 1375, 1492, 1508, 1552, 1554, 1572, 1599, 1610, 1658, 1687, 1757, 1774, 1797, 1819, 1835, 1843, 1874, 1880, 1881, 1934, 1966, 1971, 2016, 2046, 2061, 2114, 2121, 2136, 2142])\n",
            "((0, 1, 0, 1, 1), [34, 51, 52, 84, 124, 170, 192, 284, 301, 309, 352, 357, 504, 538, 541, 619, 671, 743, 776, 783, 784, 804, 846, 850, 861, 863, 871, 933, 934, 1009, 1018, 1024, 1067, 1075, 1078, 1101, 1139, 1158, 1228, 1251, 1264, 1309, 1315, 1328, 1341, 1406, 1449, 1486, 1524, 1535, 1555, 1600, 1714, 1766, 1878, 1886, 1976, 2044, 2128, 2132, 2170, 2182])\n",
            "((0, 0, 1, 1, 1), [40, 60, 70, 75, 88, 91, 131, 134, 163, 206, 221, 256, 350, 351, 356, 375, 406, 434, 436, 457, 471, 530, 561, 569, 585, 594, 599, 602, 605, 646, 676, 686, 718, 739, 748, 750, 755, 764, 772, 788, 828, 915, 930, 944, 983, 1019, 1027, 1058, 1076, 1089, 1092, 1093, 1166, 1189, 1200, 1245, 1253, 1269, 1270, 1320, 1337, 1350, 1399, 1402, 1427, 1465, 1473, 1479, 1480, 1484, 1509, 1510, 1553, 1602, 1612, 1615, 1627, 1639, 1650, 1656, 1666, 1699, 1707, 1712, 1745, 1756, 1772, 1798, 1889, 1909, 1935, 1943, 1951, 1959, 1980, 2020, 2029, 2035, 2041, 2067, 2159, 2185, 2188, 2195, 2197, 2210])\n",
            "((0, 0, 1, 0, 1), [46, 144, 189, 207, 225, 274, 298, 312, 332, 344, 368, 402, 414, 493, 499, 526, 607, 638, 648, 658, 679, 745, 752, 761, 765, 795, 807, 826, 827, 834, 901, 925, 961, 979, 989, 993, 1015, 1016, 1082, 1096, 1160, 1177, 1179, 1199, 1208, 1215, 1267, 1278, 1306, 1359, 1361, 1394, 1395, 1409, 1410, 1414, 1499, 1518, 1571, 1582, 1596, 1603, 1633, 1645, 1682, 1689, 1697, 1821, 1832, 1846, 1852, 1862, 1895, 1916, 1952, 1964, 1993, 2001, 2012, 2014, 2019, 2055, 2101, 2124, 2165, 2193, 2196, 2205])\n",
            "((1, 1, 1, 0, 1), [50, 306, 425, 465, 474, 612, 685, 740, 833, 867, 874, 919, 1031, 1121, 1123, 1186, 1327, 1439, 1443, 1452, 1644, 1649, 1676, 1693, 1694, 1708, 1724, 1748, 1790, 1823, 1831, 1833, 1872, 1877, 2073, 2111, 2152, 2184])\n",
            "((0, 0, 0, 0, 0), [53, 127, 176, 193, 201, 248, 253, 270, 339, 359, 381, 408, 410, 417, 433, 459, 476, 485, 486, 508, 523, 586, 588, 592, 593, 624, 656, 659, 661, 662, 665, 714, 725, 731, 787, 802, 820, 824, 836, 853, 898, 899, 908, 939, 945, 957, 968, 985, 1010, 1020, 1060, 1065, 1103, 1105, 1117, 1132, 1159, 1240, 1286, 1303, 1310, 1317, 1349, 1351, 1389, 1405, 1477, 1516, 1573, 1586, 1614, 1625, 1638, 1651, 1661, 1671, 1681, 1684, 1777, 1778, 1850, 1896, 1902, 1963, 2049, 2075, 2113, 2155])\n",
            "((1, 0, 1, 0, 1), [62, 69, 109, 140, 273, 320, 329, 331, 365, 373, 435, 552, 613, 636, 649, 690, 704, 707, 808, 842, 868, 956, 990, 1001, 1004, 1006, 1023, 1041, 1114, 1118, 1154, 1222, 1237, 1411, 1417, 1425, 1468, 1583, 1594, 1609, 1735, 1741, 1842, 1854, 1884, 1921, 2097, 2117, 2130, 2133, 2138, 2189])\n",
            "((1, 1, 0, 0, 0), [66, 93, 155, 214, 447, 609, 625, 716, 902, 942, 1211, 1232, 1450, 1456, 1558, 1563, 1623, 1702, 1732, 1742, 1868, 1875, 1968, 2062, 2115, 2166, 2183])\n",
            "((1, 1, 0, 1, 1), [67, 156, 194, 229, 288, 323, 327, 360, 395, 464, 481, 566, 841, 866, 891, 920, 1022, 1116, 1145, 1148, 1187, 1205, 1213, 1308, 1316, 1324, 1334, 1364, 1451, 1472, 1504, 1523, 1549, 1560, 1581, 1617, 1641, 1662, 1677, 1719, 1726, 1730, 1758, 1783, 1865, 1960, 1969, 2000, 2005, 2072, 2095, 2162])\n",
            "((1, 0, 1, 0, 0), [78, 98, 129, 167, 275, 277, 303, 317, 370, 378, 379, 478, 482, 505, 514, 596, 627, 637, 719, 733, 762, 791, 885, 936, 948, 970, 1017, 1064, 1129, 1149, 1209, 1259, 1302, 1319, 1331, 1370, 1382, 1514, 1556, 1566, 1595, 1622, 1652, 1674, 1675, 1740, 1770, 1885, 1908, 2100, 2107, 2145])\n",
            "((0, 1, 0, 0, 1), [81, 142, 209, 265, 266, 281, 391, 420, 450, 451, 483, 487, 488, 502, 510, 603, 643, 644, 653, 792, 800, 803, 854, 865, 971, 1014, 1040, 1043, 1047, 1097, 1122, 1226, 1241, 1268, 1279, 1293, 1298, 1313, 1318, 1325, 1344, 1346, 1415, 1437, 1442, 1474, 1491, 1505, 1525, 1532, 1547, 1632, 1779, 1838, 1888, 1932, 1941, 1958, 1961, 1990, 1991, 2010, 2043, 2045, 2052, 2068, 2102])\n",
            "((1, 1, 1, 0, 0), [83, 227, 257, 343, 556, 568, 589, 601, 610, 631, 634, 642, 711, 730, 823, 825, 843, 878, 927, 1053, 1090, 1285, 1464, 1616, 1657, 1680, 1688, 1710, 1721, 1891, 1998, 2042, 2110, 2146, 2194])\n",
            "((0, 0, 1, 0, 0), [101, 196, 260, 280, 285, 294, 366, 422, 444, 473, 513, 536, 550, 590, 622, 652, 721, 727, 735, 789, 835, 877, 888, 900, 918, 926, 929, 1080, 1086, 1134, 1143, 1150, 1153, 1168, 1173, 1190, 1236, 1250, 1258, 1265, 1297, 1345, 1362, 1377, 1398, 1438, 1447, 1548, 1575, 1709, 1716, 1717, 1731, 1750, 1763, 1768, 1782, 1800, 1860, 1873, 1883, 1914, 1917, 2048, 2087, 2093, 2108, 2167, 2202])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9PLKQEQWZVi",
        "outputId": "b40f6991-2a68-4537-afb1-1324c819f56d"
      },
      "source": [
        "# Vectorazing my Test data\n",
        "Test_tfidf=tfidf_vect.transform(test_data['text'].values)\n",
        "Test_processed=Test_tfidf.toarray()\n",
        "print(Test_processed.shape)\n",
        "# print(Test_processed[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IipXIhNsV-7f",
        "outputId": "117a197c-27ec-4fc6-fbc4-0ac1666b0d04"
      },
      "source": [
        "Query_pt=Test_processed[1]\n",
        "print(hashvalue(Query_pt))\n",
        "print(hash_dict[hashvalue(Query_pt)])\n",
        "print(np.linalg.norm(Query_pt))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 0, 1, 1, 0)\n",
            "[8, 13, 35, 42, 82, 85, 95, 108, 112, 115, 120, 132, 150, 157, 159, 166, 204, 217, 249, 308, 310, 311, 322, 338, 355, 362, 364, 411, 428, 445, 480, 489, 500, 503, 525, 551, 562, 626, 651, 682, 703, 705, 766, 810, 856, 881, 906, 916, 923, 937, 954, 955, 1025, 1042, 1059, 1125, 1141, 1142, 1170, 1191, 1195, 1202, 1230, 1242, 1256, 1289, 1322, 1340, 1374, 1387, 1393, 1418, 1454, 1527, 1562, 1577, 1607, 1619, 1636, 1669, 1672, 1705, 1720, 1744, 1759, 1771, 1788, 1856, 1863, 1894, 1906, 1907, 1940, 1947, 1953, 1955, 1965, 1970, 1995, 2009, 2036, 2066, 2086, 2092, 2104, 2144, 2156, 2187]\n",
            "0.9999999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_0-b6q-Ovnc",
        "outputId": "8e6adc59-d3ab-49b8-93f7-8becbb2dfb3b"
      },
      "source": [
        "#Now use the hash function on this query point and fetch all the similar data points from the hashtable.\n",
        "#Use cosine similarity to compute 11-Nearest Neighbours from the list of data points obtained in the above step\n",
        "#Defining Cosin Similarity Function\n",
        "def CosinSim(Query_pt,neigh_vect):  \n",
        "  cosin_sim=np.dot(Query_pt,neigh_vect)/(np.linalg.norm(Query_pt)*np.linalg.norm(neigh_vect))\n",
        "  return cosin_sim\n",
        "KNN_11=[]\n",
        "Final_ans=[]\n",
        "for i in range(Test_processed.shape[0]):\n",
        "  Query_pt=Test_processed[i] #Fetching the query point\n",
        "  #looping through all query point in Test data\n",
        "  test_HV=hashvalue(Test_processed[i]) #Creating Hashvalue of eacch query point\n",
        "  print(test_HV)\n",
        "  # print(hash_dict[test_HV_1])\n",
        "  All_CosSim=[]\n",
        "  neigh_pt=hash_dict[test_HV] #fetching all the neighbouring point for the query point hash value\n",
        "  for indx,pt in enumerate(neigh_pt):\n",
        "    # print(indx,pt)\n",
        "    neigh_vect=Train_processed[pt]# for each index value in neighbouring point we are finding correspondicg vector\n",
        "    \n",
        "    cosim= CosinSim(Query_pt,neigh_vect) #Finding the cosin similarity between the query point and each neighbouring point ( using loop indx)\n",
        "    All_CosSim.append((pt,cosim))\n",
        "  \n",
        "  print(\"unsorted = \",All_CosSim)\n",
        "  Sorted_AllCosSim=(sorted(All_CosSim, key = lambda x: x[1],reverse=True))[:12] # KNN usin cosin sim for KNN=11\n",
        "  temp=[train_data['category'][x[0]] for x in Sorted_AllCosSim ] \n",
        "  Final_ans.append(max(sorted(temp),key=temp.count))\n",
        "  KNN_11.append(temp)\n",
        "  \n",
        "\n",
        "print(\"KNN_11-\",KNN_11)\n",
        "print(\"Final ans-\",Final_ans)\n",
        "  \n",
        "\n",
        "  \n",
        "  # print(\"SS\",Sorted_AllCosSim)\n",
        "  # dict[test_data['category'][i]]==Sorted_AllCosSim\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 1, 1, 1, 0)\n",
            "unsorted =  [(23, 0.005262850024749239), (96, 0.01599390819941128), (105, 0.05782779241516407), (153, 0.04144110630992941), (239, 0.10942722899280966), (243, 0.0027756456362870176), (315, 0.06702712530502797), (401, 0.09309289878365656), (405, 0.01877068848038404), (468, 0.01799543898825019), (511, 0.026017069474002695), (587, 0.03312552126383352), (680, 0.004397082213909537), (694, 0.02555396015367261), (771, 0.013861905105332075), (780, 0.026206771156323635), (809, 0.008941660463043314), (847, 0.020326803035122243), (855, 0.011208754332763957), (876, 0.03189517559205342), (963, 1.0000000000000002), (973, 0.013621753338865946), (1029, 0.004438050802230254), (1113, 0.04170304183021851), (1175, 0.028404731114840827), (1223, 0.006940354131501984), (1224, 0.011607052816599128), (1378, 0.005991956612935168), (1408, 0.04513601479561055), (1416, 0.11124613300644823), (1461, 0.00581184989763069), (1481, 0.04025358563467732), (1576, 0.015601679831581072), (1665, 0.033650213277290515), (1691, 0.026366056274544716), (1899, 0.019935077591445072), (1930, 0.02161761042884795), (2022, 0.08702680027698745), (2037, 0.056929721379619776), (2171, 0.022726444822069633), (2213, 0.09309289878365656)]\n",
            "(1, 0, 1, 1, 0)\n",
            "unsorted =  [(8, 0.022186346315771304), (13, 0.05483778348215594), (35, 0.03413203796985806), (42, 0.021673441681471794), (82, 0.07808420304684677), (85, 0.05483778348215594), (95, 0.013679880754469149), (108, 0.009135577723123008), (112, 0.016837359004826365), (115, 0.004518804197354737), (120, 0.08574251775952248), (132, 0.03578859664573471), (150, 0.03861165835998441), (157, 0.04630581371894749), (159, 0.034119096146558114), (166, 0.08776993510138152), (204, 0.12409052785796479), (217, 0.0326538348193112), (249, 0.03779094090724869), (308, 0.03475776064688652), (310, 0.028690907516829678), (311, 0.047109685699103405), (322, 0.018517419592205625), (338, 0.08790465749084861), (355, 0.07694441129018262), (362, 0.04558673156800606), (364, 0.11835601650775868), (411, 0.07139262794252396), (428, 0.04937609727647793), (445, 0.026315007599496707), (480, 0.01714311701057778), (489, 0.028173092685953593), (500, 0.01610080482441971), (503, 0.03741301813120088), (525, 0.10410145334844655), (551, 0.08184782231010938), (562, 0.02853521149324249), (626, 0.030045926777322592), (651, 0.03940498063429871), (682, 0.05156390345295467), (703, 0.04457893377647984), (705, 0.04389907056617344), (766, 0.05406991916669862), (810, 0.022540176227881934), (856, 0.027132451372834474), (881, 0.020878457890935398), (906, 0.0524861247770474), (916, 0.04606914675093712), (923, 0.04061514120157147), (937, 0.061546317083269325), (954, 0.02617534186253652), (955, 0.02218913475818457), (1025, 0.03720145692561236), (1042, 0.0056181934055879205), (1059, 0.03113772418912158), (1125, 0.0820762814730729), (1141, 0.041169556442534054), (1142, 0.02427764950547897), (1170, 0.04440382722428276), (1191, 0.06831481068492658), (1195, 0.05352134638052823), (1202, 0.06345036460218112), (1230, 0.06583575583723361), (1242, 0.062383216258529973), (1256, 0.014283822602081112), (1289, 0.049045272475801135), (1322, 0.01714311701057778), (1340, 0.031047075797582174), (1374, 0.021937697406231155), (1387, 0.10674819969531202), (1393, 0.07581043967285492), (1418, 0.03448674124402785), (1454, 0.054074169794999745), (1527, 0.025084541730908832), (1562, 0.035666393916288494), (1577, 0.11412788329497105), (1607, 0.04848668681241072), (1619, 0.029571946671583936), (1636, 0.034232426193407765), (1669, 0.028690907516829678), (1672, 0.0318318153713774), (1705, 0.020033008569566007), (1720, 0.041169556442534054), (1744, 0.049986663641657374), (1759, 0.02828488016468962), (1771, 0.03746829814955726), (1788, 0.029637904511166182), (1856, 0.052237547435072), (1863, 0.017451320536240694), (1894, 0.021937697406231155), (1906, 0.0056181934055879205), (1907, 0.02706299981406732), (1940, 0.06238311936136483), (1947, 0.0877671124715065), (1953, 0.014029658047448806), (1955, 0.05179816821312923), (1965, 0.04813237241951973), (1970, 0.05094352551397069), (1995, 0.05946774837704974), (2009, 0.06020852661390343), (2036, 0.12409052785796479), (2066, 0.029235489339371194), (2086, 0.04399855510112364), (2092, 0.012550753305476949), (2104, 0.024337212356502737), (2144, 0.02854937361105118), (2156, 0.07161031428711011), (2187, 0.06831481068492658)]\n",
            "(0, 1, 0, 0, 1)\n",
            "unsorted =  [(81, 0.008050061774357745), (142, 0.18571104661517193), (209, 0.03859655666531571), (265, 0.013981264249408402), (266, 0.029165265177665614), (281, 0.019305615229959897), (391, 0.015037516986440036), (420, 0.022337684252386376), (450, 0.05127690314933403), (451, 0.022151346954381673), (483, 0.031855525924136416), (487, 0.007855043256009208), (488, 0.016048238711044445), (502, 0.029426153464486505), (510, 0.013981264249408402), (603, 0.045650832707791314), (643, 0.0692221749950079), (644, 1.0000000000000002), (653, 0.07135251362946453), (792, 0.08881645273815376), (800, 0.11512318763278281), (803, 0.09088324532030911), (854, 0.030255657814520127), (865, 0.08677509357657642), (971, 0.053276738005762374), (1014, 0.03479877236540944), (1040, 0.03575514202051283), (1043, 0.11113964149170652), (1047, 0.00544421125761922), (1097, 0.108458827978584), (1122, 0.05511331744247895), (1226, 0.0766847300499966), (1241, 0.034841579144898535), (1268, 0.06451892657365922), (1279, 0.03040910102353578), (1293, 0.016070092692858855), (1298, 0.030436604749234104), (1313, 0.024638157881027023), (1318, 0.09088324532030911), (1325, 0.108458827978584), (1344, 0.1545528026522398), (1346, 0.08037802465759816), (1415, 0.059305890402095676), (1437, 0.06410952851560112), (1442, 0.045650832707791314), (1474, 0.06520994701596691), (1491, 0.061790652296119415), (1505, 0.0654345241734116), (1525, 0.07348460918181174), (1532, 0.03687617332155258), (1547, 0.009727336003322944), (1632, 0.037529191267274505), (1779, 0.0), (1838, 0.06249959900119102), (1888, 0.033195597533689315), (1932, 0.04687976498514892), (1941, 0.07103211813891303), (1958, 0.004901377248874831), (1961, 0.0051601181802435), (1990, 0.038763307313551765), (1991, 0.11625646250792648), (2010, 0.08881645273815376), (2043, 0.13081566715254556), (2045, 0.05368896610316761), (2052, 0.02869965203988614), (2068, 0.06921903015430082), (2102, 0.02952543846818748)]\n",
            "(0, 1, 1, 0, 1)\n",
            "unsorted =  [(0, 0.05518433489402739), (36, 0.032105971738304456), (110, 0.017878003098394584), (113, 0.035415012068539184), (139, 0.015098979157226716), (169, 0.04440799089395335), (198, 0.022702457611004617), (278, 0.02807654255312044), (295, 0.022700979576844825), (412, 0.027207338991359735), (512, 0.06100344811052138), (549, 0.035415012068539184), (565, 0.021481147892461718), (573, 0.027336180068380463), (574, 0.04662270277259092), (688, 0.03898495212011323), (696, 0.041766041704556243), (712, 0.023788513101095445), (839, 0.023172679300322692), (887, 0.03217256660415124), (977, 0.10183757362844038), (1021, 0.023172679300322692), (1071, 0.0335379671820872), (1185, 0.06446499044345877), (1227, 0.07952180289821466), (1243, 0.039488627130263985), (1266, 0.0446351774908492), (1273, 0.03465346888943076), (1381, 0.03465346888943076), (1383, 0.03409533204378812), (1433, 0.031631392496854566), (1463, 0.06100344811052138), (1495, 0.04118207773974563), (1500, 0.04332768449123939), (1503, 0.016584377472660162), (1526, 0.027376819192568257), (1550, 0.04726285948106395), (1584, 0.045930281292457686), (1814, 0.04142190364195444), (1849, 0.049056849719493624), (1864, 0.04726285948106395), (2032, 0.028670124373840283), (2040, 0.04060098600676768), (2051, 0.006906736158959865), (2131, 0.011646229564973739), (2137, 0.05399698181622053), (2151, 0.03898495212011323)]\n",
            "(1, 0, 1, 1, 0)\n",
            "unsorted =  [(8, 0.007762628170762946), (13, 0.01417034709543439), (35, 0.03530730308283635), (42, 0.021010260300747565), (82, 0.008037226712273445), (85, 0.01417034709543439), (95, 0.005143561028657072), (108, 0.01823040273818743), (112, 0.012248781813426216), (115, 0.008666904777585128), (120, 0.012729571885487423), (132, 0.021963362169325203), (150, 0.014286967128083972), (157, 0.01389566748568443), (159, 0.0262641052345353), (166, 0.01880079941026402), (204, 0.032504159048636766), (217, 0.0396058350876317), (249, 0.02635196113860171), (308, 0.05661994504936792), (310, 0.02984339484840414), (311, 0.004412827208893644), (322, 0.05912775946524769), (338, 0.023571738789049418), (355, 0.04089399584543131), (362, 0.023387888906145172), (364, 0.02872916689660858), (411, 0.024028177507153067), (428, 0.07326328942088552), (445, 0.01786240113390987), (480, 0.01610465394328676), (489, 0.050994734876308775), (500, 0.009222221269897996), (503, 0.014203404598037504), (525, 0.024662112171486957), (551, 0.006479989257602604), (562, 0.011106480726424684), (626, 0.04200824623015858), (651, 0.011782962287492952), (682, 0.04362699843765283), (703, 0.0390827424037289), (705, 0.04364276971516339), (766, 0.03284147407604403), (810, 0.004035648058363088), (856, 0.008732176970645836), (881, 0.030604954811009902), (906, 0.043060557616554127), (916, 0.030179711266642012), (923, 0.031058536869931968), (937, 0.07562754815122527), (954, 0.053339607531871676), (955, 0.06522287303000669), (1025, 0.051192862104497076), (1042, 0.020129427577737686), (1059, 0.0031377069855901395), (1125, 0.023321775169576796), (1141, 0.024373585503456176), (1142, 0.05190941520272714), (1170, 0.021989590062677093), (1191, 0.025670709965453523), (1195, 0.013274367368713237), (1202, 0.047603974381450684), (1230, 0.06337514127472134), (1242, 0.04513290670613874), (1256, 0.021188111553243398), (1289, 0.01925453414968994), (1322, 0.01610465394328676), (1340, 0.04748683190137155), (1374, 0.012274385967435547), (1387, 0.018820047747664888), (1393, 0.0638555726467694), (1418, 0.05900681873417955), (1454, 0.08008473185296995), (1527, 0.019240785941507552), (1562, 0.07804453543391841), (1577, 0.03771559268973421), (1607, 0.036609847927657355), (1619, 0.02250289930380513), (1636, 0.027440301317371175), (1669, 0.02984339484840414), (1672, 0.006163259904929129), (1705, 0.01671754613873133), (1720, 0.024373585503456176), (1744, 0.04116315992726927), (1759, 0.023529976425839382), (1771, 0.03935951616774072), (1788, 0.01939811753194503), (1856, 0.02429419311835836), (1863, 0.029111083819850683), (1894, 0.012274385967435547), (1906, 0.020129427577737686), (1907, 0.008005622445411175), (1940, 0.016345431852541968), (1947, 0.028481878641841752), (1953, 0.06150057221603774), (1955, 0.06376414036478534), (1965, 0.04976837852062791), (1970, 0.055231612676578584), (1995, 0.04551115587847437), (2009, 0.023998022285070336), (2036, 0.032504159048636766), (2066, 0.012795295026118193), (2086, 0.04768424442838398), (2092, 0.006771840413330909), (2104, 0.024953119784854927), (2144, 0.03530799272618811), (2156, 0.06481096385279694), (2187, 0.025670709965453523)]\n",
            "(0, 0, 1, 1, 0)\n",
            "unsorted =  [(7, 0.054384092358270054), (33, 0.0036487996457489763), (61, 0.007133086244192613), (76, 0.021860908125671343), (89, 0.08779490809512944), (100, 0.03927677823857317), (111, 0.022759687412118607), (126, 0.03812874821308579), (178, 0.0038476232666815064), (184, 0.015219831402455912), (191, 0.005986681102772959), (211, 0.023932109700937863), (237, 0.08726653723965454), (276, 0.02620293033097877), (290, 0.014895396609766727), (302, 0.001765744631514141), (305, 0.003000652661349465), (307, 0.004217210962104127), (340, 0.06600942251459774), (348, 0.004983809679269545), (461, 0.0049152193161795255), (518, 0.019365997169977535), (581, 0.01275055883735519), (606, 0.006168639416596905), (620, 0.001326532781464856), (675, 0.004492641959600576), (684, 0.0037757307267213564), (691, 0.03525431059960655), (693, 0.002061100105717733), (710, 0.07040933597389523), (829, 0.03604564898812126), (893, 0.024097717218750068), (913, 0.031325481828184114), (924, 0.0770778097138063), (928, 0.019057856935187932), (938, 0.01780900049798197), (946, 0.0137923069870718), (986, 0.01452810218353519), (1012, 0.031520328722974035), (1039, 0.018382465284486782), (1079, 0.09797702544914537), (1104, 0.049126188127068544), (1115, 0.023113126682153897), (1137, 0.0017204225003640784), (1147, 0.047923002724277644), (1152, 0.03902572825716237), (1207, 0.08782216607280983), (1248, 0.06559722262076077), (1249, 0.02697213280723272), (1252, 0.008589725845945091), (1276, 0.09001474985090338), (1304, 0.15235319492921517), (1307, 0.04252535554058811), (1357, 0.009933927207730777), (1369, 0.007939641599492961), (1392, 0.028798089748600413), (1403, 0.10943161911311258), (1435, 0.0440772290689974), (1459, 0.01636515029488484), (1466, 0.0012489617575353366), (1494, 0.003563279102140207), (1520, 0.01780900049798197), (1540, 0.006381636011391174), (1591, 0.02170781646820174), (1604, 0.007410108894445471), (1618, 0.027933750610446315), (1620, 0.018729327131371452), (1663, 0.010969533093539697), (1667, 0.003999881001947619), (1692, 0.005187464409030717), (1746, 0.21454932887944775), (1747, 0.00171972811930094), (1767, 0.06224181983246292), (1791, 0.04289262305121989), (1795, 0.0), (1806, 0.03340786586337619), (1815, 0.06106087579643956), (1851, 0.011447392642418472), (1857, 0.006446388470070294), (1869, 0.06738836741543085), (1882, 0.02881885328492493), (1915, 0.07053895673250926), (1927, 0.036008086540746026), (1942, 0.03924516535296169), (1956, 0.0654096385781188), (1997, 0.002048098252118482), (2024, 0.030064303304900647), (2038, 0.006928924452558239), (2060, 0.048517227331671325), (2071, 0.01606155541784579), (2076, 0.06600942251459774), (2082, 0.01727223693955941), (2106, 0.0376136050859491), (2123, 0.01970889954984005), (2158, 0.03367078178429454), (2173, 0.006688843545303065), (2206, 0.006928924452558239), (2209, 0.043526271463978757), (2212, 0.01840095155491345)]\n",
            "(0, 1, 0, 1, 0)\n",
            "unsorted =  [(5, 0.05814435568958943), (9, 0.03637619339840738), (12, 0.011454019171836326), (55, 0.03523360661306264), (177, 0.0010338563003435392), (182, 0.009866203303952945), (215, 0.02566188974208262), (264, 0.05054121154575369), (292, 0.012217121419151315), (328, 0.031065447448474018), (349, 0.008385922285121514), (441, 0.005113794533509096), (448, 0.0), (453, 0.009459424201758338), (517, 0.0007063355610472465), (554, 0.015998018711818433), (570, 0.020210173376924916), (598, 0.0030443802881858924), (647, 0.0777325510492897), (660, 0.03593227733518602), (681, 0.0008995400616245056), (786, 0.005345404354802455), (805, 0.01647650138335895), (821, 0.01749908739526224), (859, 0.026198772706519536), (860, 0.0237541644558779), (1063, 0.004385682347547434), (1151, 0.0011014273723572352), (1206, 0.008385922285121514), (1413, 0.014709844751890194), (1611, 0.01647650138335895), (1647, 0.008930612977042937), (1685, 0.0008577350015468548), (1698, 0.040737316074040304), (1738, 0.005542498450730816), (1784, 0.0020173532243884904), (1803, 0.056974806058132085), (1830, 0.05054121154575369), (1839, 0.02314650295491944), (1901, 0.011329357515996945), (1905, 0.009891947830882843), (1950, 0.010647063852281978), (1957, 0.0013142620454186518), (1979, 0.009578628052253834), (1999, 0.0), (2028, 0.010316823544760104), (2122, 0.02084403863677413), (2148, 0.003197071824148197), (2164, 0.003279262821868186), (2211, 0.05410040169713832)]\n",
            "(1, 0, 0, 1, 1)\n",
            "unsorted =  [(4, 0.08211426967098823), (44, 0.039335108425363294), (56, 0.061628222928687505), (90, 0.08366159998823755), (133, 0.017626927604455323), (143, 0.032639645434329956), (173, 0.04890982547424685), (175, 0.03449260860269972), (183, 0.05473634838816124), (200, 0.03865271083303213), (212, 0.018286622967832008), (222, 0.033406853296206705), (240, 0.028635407782339677), (297, 0.018634819792905852), (299, 0.04136696987004112), (316, 0.04759873744820533), (358, 0.029642062478978794), (361, 0.02850597717645467), (372, 0.10035464587094485), (404, 0.03685577830291025), (413, 0.05592580863982712), (430, 0.0561217417118917), (463, 0.06742385814336346), (467, 0.023188077984737092), (494, 0.026925530851078204), (520, 0.01862542740498462), (555, 0.0514470082780778), (595, 0.07175484440401375), (632, 0.02850597717645467), (650, 0.029957689858144), (664, 0.06110678799696023), (677, 0.05926694651350536), (769, 0.010015671259398963), (778, 0.0561217417118917), (801, 0.06144601440473693), (816, 0.047112210786812346), (818, 0.05558096340730604), (822, 0.030583811665022907), (858, 0.0559929934520758), (869, 0.05178258850721715), (872, 0.018745457974941246), (882, 0.04525187497953673), (895, 0.051633795232948025), (910, 0.08389367544120879), (965, 0.041116961433997325), (969, 0.02092301698880482), (976, 0.05687564265012088), (978, 0.0027232209295286283), (980, 0.029696425694094447), (996, 0.0346505435937439), (1026, 0.036594830742048536), (1035, 0.09852757085358978), (1044, 0.03925682679241742), (1050, 0.05038963452568111), (1069, 0.04558314841798438), (1098, 0.020967433009227647), (1108, 0.02684326249074259), (1130, 0.07673071022450163), (1135, 0.038373359981013926), (1244, 0.017456005415966997), (1348, 0.06788618587814356), (1356, 0.022960755461597985), (1388, 0.012674340929654963), (1448, 0.07033196962825806), (1501, 0.07986605519428762), (1506, 0.04716852946190611), (1512, 0.01714452849611987), (1530, 0.011376507549114689), (1543, 0.029281921843662347), (1567, 0.044338619949646677), (1597, 0.03078097663127433), (1606, 0.045670457697203334), (1700, 0.025473330966202305), (1704, 0.049220092830352315), (1706, 0.1580781484194103), (1723, 0.009368547039549945), (1733, 0.0379802120650552), (1752, 0.08452532388970294), (1799, 0.10294145285646426), (1809, 0.020845632463241198), (1826, 0.030333135222876895), (1841, 0.030157139061725423), (1853, 0.08211426967098823), (1887, 0.04727870518031434), (1903, 0.03185431143938665), (1911, 0.03316059625678448), (1918, 0.04217714472047969), (1923, 0.021287135999331512), (1925, 0.023598392720455854), (1967, 0.049220092830352315), (1972, 0.08326816394954935), (1975, 0.03341571169423313), (1978, 0.0177093974005287), (1989, 0.043149180258613425), (2006, 0.03969630071956071), (2023, 0.045141807108558925), (2027, 0.09773549789289261), (2053, 0.03925682679241742), (2054, 0.03138213647502082), (2109, 0.04592625298231558), (2141, 0.042365744512363614), (2174, 0.03341571169423313), (2198, 0.05227342103340733), (2207, 0.025473330966202305)]\n",
            "(1, 0, 0, 0, 1)\n",
            "unsorted =  [(22, 0.06308227359567953), (38, 0.0660069429416595), (39, 0.046639613516904946), (79, 0.050122693299053465), (119, 0.05458273055477385), (147, 0.0014521927027295078), (179, 0.03500891425021901), (223, 0.04791812230811837), (231, 0.03900206657702565), (251, 0.05789171276506958), (263, 0.06432804470004493), (271, 0.025599403974559027), (313, 0.05215018164928466), (335, 0.04447642169148315), (341, 0.10026338084374105), (369, 0.01354663937642985), (376, 0.012390962270870077), (407, 0.026174518667059315), (419, 0.1245338873814322), (440, 0.028224136874159464), (560, 0.015323029670380166), (567, 0.13024527720974313), (572, 0.03816769390316656), (611, 0.03176297202181671), (667, 0.027052378369186433), (851, 0.07149461019015314), (884, 0.06503181286614874), (894, 0.10063287221241934), (903, 0.05482135195901885), (914, 0.024068166074720176), (962, 0.042468336423877276), (987, 0.055152321025653504), (1051, 0.06451626601021698), (1068, 0.059937230185012194), (1107, 0.10373968563617211), (1127, 0.07948619278407014), (1138, 0.054487489697421294), (1157, 0.10063287221241934), (1181, 0.025599403974559027), (1182, 0.052678907303839125), (1192, 0.07730764045890985), (1212, 0.046526241596456665), (1246, 0.021783421572711535), (1263, 0.021276540605164554), (1305, 0.07482798436831353), (1323, 0.03293371590298166), (1342, 0.011960244100398185), (1421, 0.033257735531663356), (1434, 0.03995400565093025), (1458, 0.09851848043231219), (1483, 0.02507864170003661), (1515, 0.08844731997038652), (1534, 0.008893241791411412), (1578, 0.043561366561725455), (1587, 0.04137682087245457), (1626, 0.01758961463837332), (1635, 0.06503181286614874), (1660, 0.0660069429416595), (1695, 0.04985151495282206), (1825, 0.0844214047436196), (1844, 0.07216893786423591), (1867, 0.028554625074832005), (1945, 0.06141884536860375), (2002, 0.04273176342755604), (2033, 0.0137009644110054), (2039, 0.03750413376111811), (2059, 0.16313021140880055), (2098, 0.01854988710682452), (2099, 0.02450535076902299), (2154, 0.04765790835759236), (2178, 0.04696928368989261), (2179, 0.08409391091809773), (2203, 0.03652833988757616), (2214, 0.08086409342886561)]\n",
            "(1, 0, 0, 1, 0)\n",
            "unsorted =  [(20, 0.008370961089828955), (25, 0.0573568141890544), (41, 0.12072101008784453), (43, 0.026703655803385233), (87, 0.00856560027688187), (99, 0.08299842952610752), (102, 0.04343042247687512), (130, 0.06392794830823263), (136, 0.028555782783195875), (171, 0.019651342119542562), (216, 0.05337981219898525), (218, 0.0560138261292159), (262, 0.028651261951183737), (272, 0.031320399576929385), (304, 0.06783875875883842), (325, 0.011187083362882454), (330, 0.008120613858489969), (367, 0.07264101824999086), (380, 0.03494820442121807), (383, 0.03300809770024939), (396, 0.02597691220649898), (437, 0.04361557395660278), (438, 0.07662440727530992), (458, 0.022949018125023413), (497, 0.02772376271427697), (506, 0.06970560374936374), (522, 0.034447830680292425), (527, 0.01668855768710007), (546, 0.08832235903076807), (557, 0.08517112968924447), (577, 0.12736129421743564), (628, 0.045554592125338175), (640, 0.0032810107577602677), (698, 0.08954955321246469), (708, 0.02052855215448375), (734, 0.05203801299285668), (742, 0.03170878027962245), (746, 0.004813945177575551), (749, 0.014966862187707142), (768, 0.03070080093432729), (775, 0.06756223401584534), (779, 0.04838233958896202), (798, 0.08899964571399173), (799, 0.04295433601028008), (811, 0.0593016159445243), (815, 0.077855755099448), (819, 0.023113503105209984), (838, 0.020466848965737715), (840, 0.03486486627499673), (904, 0.025182722141088174), (972, 0.06392794830823263), (975, 0.033481949622873076), (981, 0.016452326430493554), (982, 0.062238586714650754), (984, 0.05619997712578126), (1046, 0.08338633024823347), (1081, 0.05118978976281738), (1112, 0.057768498835385935), (1133, 0.033387957433103466), (1144, 0.017056325287686927), (1167, 0.019303810425381426), (1184, 0.08826422613854137), (1203, 0.030883855391972994), (1239, 0.05261084669214406), (1287, 0.06903314088836865), (1314, 0.03145958279087834), (1321, 0.02935840440017568), (1352, 0.02738039542290351), (1386, 0.023060482862598774), (1391, 0.020832023165656743), (1412, 0.017056879104803847), (1420, 0.05470765706949144), (1432, 0.02655530394099425), (1467, 0.030883855391972994), (1469, 0.07473190696945425), (1470, 0.0458708300455937), (1511, 0.11916534180521465), (1517, 0.0032810107577602677), (1529, 0.05414149269527781), (1561, 0.03250822561205875), (1574, 0.054255420068121854), (1605, 0.01720537903276023), (1628, 0.051548473151966964), (1631, 0.01552943019962308), (1678, 0.00659678605568559), (1679, 0.008370961089828955), (1736, 0.03149542321508527), (1739, 0.0350893493655423), (1801, 0.0491776410013995), (1817, 0.03134905598902753), (1822, 0.05820856976705217), (1924, 0.07678083327552422), (1928, 0.028020640766033576), (1939, 0.11137672886612347), (1946, 0.014927413236276086), (1982, 0.03354995942202795), (1987, 0.01542101628393734), (2004, 0.08368185613462976), (2017, 0.01720537903276023), (2050, 0.011013175095570786), (2064, 0.020083390092244177), (2139, 0.018491425105260997), (2150, 0.020466848965737715), (2168, 0.011331344956964686), (2172, 0.12593374808772884), (2175, 0.03730070874538794), (2180, 0.027443303821590583), (2181, 0.0376197443705716), (2190, 0.057147143310844854), (2200, 0.01990821075959785)]\n",
            "KNN_11- [['tech', 'tech', 'tech', 'tech', 'tech', 'business', 'sport', 'tech', 'business', 'tech', 'tech', 'business'], ['entertainment', 'entertainment', 'entertainment', 'politics', 'entertainment', 'business', 'business', 'entertainment', 'business', 'entertainment', 'business', 'entertainment'], ['tech', 'tech', 'tech', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech'], ['sport', 'politics', 'tech', 'tech', 'tech', 'tech', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics'], ['tech', 'entertainment', 'tech', 'business', 'business', 'sport', 'tech', 'sport', 'business', 'business', 'business', 'politics'], ['business', 'business', 'business', 'business', 'sport', 'entertainment', 'business', 'business', 'business', 'business', 'business', 'sport'], ['politics', 'politics', 'entertainment', 'entertainment', 'tech', 'tech', 'tech', 'entertainment', 'politics', 'politics', 'business', 'tech'], ['entertainment', 'politics', 'entertainment', 'politics', 'entertainment', 'entertainment', 'entertainment', 'sport', 'business', 'entertainment', 'entertainment', 'tech'], ['politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'entertainment', 'politics', 'business', 'sport', 'entertainment', 'business'], ['sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'tech', 'business', 'sport', 'sport']]\n",
            "Final ans- ['tech', 'entertainment', 'tech', 'tech', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "24sUvtiUZOYA",
        "outputId": "d0e77e65-d946-43f2-dfd6-8240c7f0eaec"
      },
      "source": [
        "a=['tech','tech','business', 'business','politics','sport', 'sport', 'entertainment']\n",
        "max(sorted(a),key=a.count)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69lhueTMVJnb",
        "outputId": "34eb5fe9-6d31-457e-b3ff-b92b3565f80b"
      },
      "source": [
        "###########################################\n",
        "## GRADER CELL: Do NOT Change this.\n",
        "# This cell will print \"Success\" if your implmentation of the predictLabels() is correct and the accuracy obtained is above 80%.\n",
        "# Else, it will print \"Failed\"\n",
        "###########################################\n",
        "import numpy as np\n",
        "\n",
        "# Predict the labels using the predictLabels() function\n",
        "Y_custom = np.array(Final_ans)\n",
        "\n",
        "# Reference grader array - DO NOT MODIFY IT\n",
        "Y_grader = np.array(['tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport'])\n",
        "\n",
        "# Calculating accuracy by comparing Y_grader and Y_custom\n",
        "accuracy = np.sum(Y_grader==Y_custom) * 10\n",
        "\n",
        "if accuracy >= 80:\n",
        "  print(\"******** Success ********\",\"Accuracy Achieved = \", accuracy,'%')\n",
        "else:\n",
        "  print(\"####### Failed #######\",\"Accuracy Achieved = \", accuracy,'%')\n",
        "  print(\"\\nY_grader = \\n\\n\", Y_grader)\n",
        "  print(\"\\n\",\"*\"*50)\n",
        "  print(\"\\nY_custom = \\n\\n\", Y_custom)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******** Success ******** Accuracy Achieved =  90 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhnngvQkrnBB"
      },
      "source": [
        "## Grader Cell\n",
        "\n",
        "Please execute the following Grader cell to verify the correctness of your above implementation. This cell will print \"Success\" if your implmentation of the predictLabels() is correct, else, it will print \"Failed\". Make sure you get a \"Success\" before you submit the code in the classroom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX1sji2XrtmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea35b7a-7e26-4c43-9d3a-ce5bd61e4df6"
      },
      "source": [
        "###########################################\n",
        "## GRADER CELL: Do NOT Change this.\n",
        "# This cell will print \"Success\" if your implmentation of the predictLabels() is correct and the accuracy obtained is above 80%.\n",
        "# Else, it will print \"Failed\"\n",
        "###########################################\n",
        "import numpy as np\n",
        "\n",
        "# Predict the labels using the predictLabels() function\n",
        "Y_custom = np.array(predictLabels(test_data))\n",
        "\n",
        "\n",
        "# Reference grader array - DO NOT MODIFY IT\n",
        "Y_grader = np.array(['tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport'])\n",
        "\n",
        "# Calculating accuracy by comparing Y_grader and Y_custom\n",
        "accuracy = np.sum(Y_grader==Y_custom) * 10\n",
        "\n",
        "if accuracy >= 80:\n",
        "  print(\"******** Success ********\",\"Accuracy Achieved = \", accuracy,'%')\n",
        "else:\n",
        "  print(\"####### Failed #######\",\"Accuracy Achieved = \", accuracy,'%')\n",
        "  print(\"\\nY_grader = \\n\\n\", Y_grader)\n",
        "  print(\"\\n\",\"*\"*50)\n",
        "  print(\"\\nY_custom = \\n\\n\", Y_custom)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4000)\n",
            "******** Success ******** Accuracy Achieved =  90 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}