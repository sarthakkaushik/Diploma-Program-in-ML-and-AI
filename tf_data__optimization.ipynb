{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.data _optimization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1vpwbHMqEmdUe5yxidF30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthakkaushik/Diploma-Program-in-ML-and-AI/blob/main/tf_data__optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vzUZpkPp6t60"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(record):\n",
        "\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CkKqkRFW7m-_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= tf.data.TFRecordDataset(\".../*tfrecord\") # This will read the data\n",
        "\n",
        "#TFRecordDataset contructor that the set of file name or patterns and produces eleents that are stored in that file in sequcen like manner\n",
        "#Once you create the dataste you can tranform the dataset by using user defiened fucntion using MAP function.\n",
        "\n",
        "dataset=dataset.map(preprocess)\n",
        "\n",
        "# now we can apply batch tranform which take multiple elements of oinput datset and outputs single element as an output that has higher dimension. this is used to create efficeiny \n",
        "\n",
        "dataset=dataset.batch(batch_size=32)"
      ],
      "metadata": {
        "id": "054MgAAe7vOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE- map tranformation followed up by batch tranformation can give 2x speed. This is how this library is implemented "
      ],
      "metadata": {
        "id": "5xN0rXaO9M76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to improve Input Piple Performance ?"
      ],
      "metadata": {
        "id": "S6Vj43Yd-QFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use prefecth fuctionality tf dataset\n",
        "dataset= tf.data.TFRecordDataset(\".../*tfrecord\")\n",
        "dataset=dataset.map(preprocess)\n",
        "dataset=dataset.batch(batch_size=32)\n",
        "#adding prfecth so that both CPU and GPU resource are used efficiently\n",
        "dataset= dataset.prefecth(buffer_size=X )"
      ],
      "metadata": {
        "id": "JL7PPae2-Xsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parallel Tranformation - to improve input pipline performance even more. You can overlap the preprocessing of all the individual elemets for which we are going to creat a batch out of.\n"
      ],
      "metadata": {
        "id": "H-1KFqpi_C8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is pretty simple - you just add a single argument called - num_parallel_calls to the \"map\" tranformation , which indicate to the tf.data runtime that it should infact\n",
        "#preprocess elemnets of the input dataset in parallel. User dont need to take care of multithreading multiprocess python API. This will done automatically oe be aware og things of \n",
        "# gloabl intepreter log, it just happens inside tf.data runtime which is implemented in C++ and this it sidesteps the complexities that user have and need to go through.\n",
        "\n",
        "dataset= tf.data.TFRecordDataset(\".../*tfrecord\")\n",
        "dataset=dataset.map(preprocess, num_parallel_calls=Y)\n",
        "dataset=dataset.batch(batch_size=32)\n",
        "dataset= dataset.prefecth(buffer_size=X )"
      ],
      "metadata": {
        "id": "_6Ipsf6M_tjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paralle Extraction- Last best practice to improve optimization\n",
        "\n",
        "\n",
        "> So similar to parallel tranformation , where in that case the sequential mapping of the data might have been the bottele neck. Another potential source of your input pipline is the sequential nature by which the data is being read.\n",
        "If you are reading elments from afile one at a time , the I/O could actually be bottleneck of your pipline\n",
        "\n",
        "Answer- you dont need to do it sequentially , you can do it in parallel.To do that you need to remove TFREOCORDSDAtASET and add a two line change.\n",
        "\n"
      ],
      "metadata": {
        "id": "QyMLr-hDA20b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= tf.data.Dataset.list_files(\".../*tfrecord\") # this read the path of all the file names by matching the patterns which we defined\n",
        "\n",
        "# Now we are going to apply \"interleve\" tranformation to this dataset which take a user defined fucn.whihc is dataset factory operating on inputs ( in this case the file names) and this will create tf dataset for that particular file name\n",
        "# and specifying num_parallel_calls argument will determine how many files should we be reading in parallel at any given time.\n",
        "dataset=dataset.interleave(TFRecordDataset,num_parallel_calls=Z)\n",
        "dataset=dataset.map(preprocess, num_parallel_calls=Y)\n",
        "dataset=dataset.batch(batch_size=32)\n",
        "dataset= dataset.prefecth(buffer_size=X )"
      ],
      "metadata": {
        "id": "gncrbmloFUAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to determine optimal value for X, Y, Z based on resource/hardware you are using - you can use tf.data.experimental.AutoTune"
      ],
      "metadata": {
        "id": "wAsKKQ1BH2Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= tf.data.Dataset.list_files(\".../*tfrecord\")\n",
        "dataset=dataset.interleave(TFRecordDataset,num_parallel_calls=tf.data.experimental.AutoTune)\n",
        "dataset=dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AutoTune)\n",
        "dataset=dataset.batch(batch_size=32)\n",
        "dataset= dataset.prefecth(buffer_size=tf.data.experimental.AutoTune )"
      ],
      "metadata": {
        "id": "m7gUgtM_FHGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GNC5Umr8BKHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TDoi1Kqz_CAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ivJXI0Wb-UvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZsxKRkmY-O6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8zcsSDQl-MaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3hyC1axC9XBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w6Qkc_Qb7sqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}